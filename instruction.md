
# ðŸ“± iOS App Development Guide

## ðŸ” Project Overview
This iOS app helps users **analyze food items through image capture** and receive **detailed nutritional insights** using AI-based image recognition. The goal is to promote healthy dietary choices with a delightful user experience.

## ðŸ—‚ Project Structure
```
iOSApp-VibeCodingTest/
â”œâ”€â”€ App/                 # App configuration and entry point
â”œâ”€â”€ Features/            # Main app features
â”‚   â”œâ”€â”€ Home/            # Dashboard and daily summary
â”‚   â”œâ”€â”€ Camera/          # Camera integration and image analysis
â”‚   â”œâ”€â”€ Analysis/        # Trends, charts, and history
â”‚   â”œâ”€â”€ MainTab/         # Tab bar controller
â”‚   â”œâ”€â”€ Settings/        # Goal management and app preferences
â”‚   â””â”€â”€ Login/           # User onboarding and authentication
â”œâ”€â”€ Models/              # SwiftData and response models
â”œâ”€â”€ Shared/              # Reusable views, styles, and utilities
â””â”€â”€ Resources/           # Assets and localized strings
```

## ðŸ§± Development Guidelines

### ðŸ§© Architecture
- âœ… Follow **MVVM** pattern (`@Observable` for state)
- âœ… Use **SwiftUI** and **SwiftData** (iOS 18.4+)
- âœ… Apply **clean architecture**: views, view models, models, services

### ðŸŽ¨ Code Style
- Follow Apple's **Swift API Design Guidelines**
- Use **descriptive names** for clarity
- Add comments for non-obvious logic
- Prefer **smaller, reusable views and functions**
- Use **`enum Constants`** for static strings and values
- Emphasize **compile-time safety** with strong typing

### ðŸ“· Camera Module
- Request **NSCameraUsageDescription** in `Info.plist`
- Use **AVFoundation** to capture images
- Handle permissions and errors gracefully
- Compress images before sending to API
- Support real-device only (not supported on Simulator)

### ðŸ“Š Analysis Module
- Send images to **OpenAI Vision API** (`gpt-4-vision-preview`)
- Parse JSON response into `VisionAPIResponse`
- Provide editable fields (calories, macros)
- Allow saving analyzed data to SwiftData

### ðŸ‘¤ Authentication Module
- Basic login support
- Save user preferences locally using `SwiftData`
- Future-ready for iCloud sync or backend integration

## ðŸš€ Performance Best Practices
- Use `@MainActor` and `Task.detached` appropriately
- Compress and debounce image uploads
- Avoid blocking UI with long operations
- Optimize SwiftUI rendering (e.g., LazyVStack)

## ðŸ” Security
- Use `HTTPS` for all external requests

## ðŸ›  Required Dependencies
- `SwiftUI` (UI layer)
- `AVFoundation` (camera)
- `SwiftData` (local persistence)
- `Charts` (visual analytics)
- `OpenAI` GPT-4 Vision API
- `Cursor.dev` (AI-based development environment)
- `SweetPad` (for running app on iOS device from Cursor)

## ðŸ§ª Build & Run Instructions
```bash
git clone https://github.com/pratyushpratiksinha/iOSApp-VibeCodingTest.git
cd iOSApp-VibeCodingTest
open iOSApp-VibeCodingTest.xcodeproj
```

> Build and run the project.

> âš ï¸ **Note:**  
> This is a **camera-based application** and requires a **real iOS device** to run.  
> The **iOS Simulator does not support camera input**.  
> Ensure your device is connected and selected in Xcode before building.


## ðŸ¤ Contributing
1. Fork this repo  
2. Create a branch (`git checkout -b feature/FixCameraBug`)  
3. Commit changes (`git commit -m "fix: resolved crash when camera is denied"`)  
4. Push and open a Pull Request  

## ðŸ“¬ Support
If you have any questions, ideas, or bugs to report:
> **Email:** [pratyushpratiksinha@gmail.com](mailto:pratyushpratiksinha@gmail.com)
